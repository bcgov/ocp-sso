## STATE SYNC
# Syncs two backup container pvcs together to be used as the primary source of state
# synconrizations between two different openshift cluster rh sso db deployments
# NOMENCLATURE
# TARGET CLUSTER: the target cluster is the receiver of any state
# SOURCE CLUSTER: where the state is being sync'd from
---
  - hosts: localhost
    gather_facts: no
    vars_prompt:
    # can skip these by running playbook with --extra-vars
      - name: "has_cluster_contexts"
        prompt: "have you logged into {{ migrator_vars.cluster_a.server_url }} and {{ migrator_vars.cluster_b.server_url }}?"
        private: no
    vars_files:
      - group_vars/all/vars.yml
    tasks:
      - when: not has_cluster_contexts|bool
        debug:
          msg: "Must be logged into both clusters as well as have had run a backup in cluster A"
      - failed_when: not has_cluster_contexts|bool
        shell: echo "Starting!"
      - name: Pre requisites
        include_tasks: ./tasks/prereqs.yml

      - name: Login to Cluster A
        shell: oc config use-context {{ migrator_vars.cluster_a.context }}
      - name: Scaling Cluster A Services to 0
        include_tasks: ./tasks/scale_down_services.yml
        vars:
          TARGET_NAMESPACE: "{{ migrator_vars.cluster_a.namespace }}"
          SSO_DC_NAME: "{{ migrator_vars.cluster_a.sso_deployment_config }}"
          SSO_BACKUP_NAME: "{{ migrator_vars.cluster_a.backup_deployment_config }}"
          SSO_HPA_NAME: "{{ migrator_vars.cluster_a.sso_hpa_name }}"

      - name: Login to Cluster B
        shell: oc config use-context {{ migrator_vars.cluster_b.context }}
      - name: Scaling Cluster B Services to 0
        include_tasks: ./tasks/scale_down_services.yml
        vars:
          TARGET_NAMESPACE: "{{ migrator_vars.cluster_b.namespace }}"
          SSO_DC_NAME: "{{ migrator_vars.cluster_b.sso_deployment_config }}"
          SSO_BACKUP_NAME: "{{ migrator_vars.cluster_b.backup_deployment_config }}"
          SSO_HPA_NAME: "{{ migrator_vars.cluster_b.sso_hpa_name }}"
      - debug:
          msg: Beginning Migration Process
      
      - name: Deploying Source Migrator to Cluster B
        include_tasks: ./tasks/deploy_cluster_b_migrator.yml

      # FACTS SET AT THIS POINT:
      # - CLUSTER_B_SA_SECRET: ansible built in file path that contains secret manifest
      # - CLUSTER_B_SA_SECRET_NAME: the secret name that can be queried when the manifest is applied
      - name: Deploying Source Migrator to Cluster A
        include_tasks: ./tasks/deploy_cluster_a_migrator.yml
      # FACTS SET AT THIS POINT:
      # - CLUSTER_A_MIGRATOR_POD: the pod name to be used as host in later plays
      # - CLUSTER_B_MIGRATOR_POD: the pod name to be used as host in later plays
      - name: Getting host for migrator pod
        include_tasks: ./tasks/get_cluster_hosts.yml
      
      - name: Rsync'n the state between Cluster A and Cluster B
        include_tasks: tasks/rsync.yml

      - name: Login to Cluster A
        shell: oc config use-context {{ migrator_vars.cluster_a.context }}
      - name: Killing Migrator Pods
        include_tasks: ./tasks/cleanup.yml
        vars:
          TARGET_NAMESPACE: "{{ migrator_vars.cluster_a.namespace }}"
          MIGRATOR_POD_SELECTOR: "app={{ migrator_vars.cluster_a.migrator.name }}-{{ migrator_vars.cluster_a.name }}"
      - name: Bring up backup container
        include_tasks: tasks/scale_up_backup_container.yml
        vars:
          TARGET_NAMESPACE: "{{ migrator_vars.cluster_a.namespace }}"
          SSO_BACKUP_NAME: "{{ migrator_vars.cluster_a.backup_deployment_config }}"
      - debug:
          msg: Completed cleanup of Cluster A. Migrator Deployment deleted and Backup brough back up

      - name: Login to Cluster B
        shell: oc config use-context {{ migrator_vars.cluster_b.context }}
      - name: Killing Migrator Pods
        include_tasks: tasks/cleanup.yml
        vars:
          TARGET_NAMESPACE: "{{ migrator_vars.cluster_b.namespace }}"
          MIGRATOR_POD_SELECTOR: "app={{ migrator_vars.cluster_b.migrator.name }}-{{ migrator_vars.cluster_b.name }}"
      - name: Bring up backup container
        include_tasks: tasks/scale_up_backup_container.yml
        vars:
          TARGET_NAMESPACE: "{{ migrator_vars.cluster_b.namespace }}"
          SSO_BACKUP_NAME: "{{ migrator_vars.cluster_b.backup_deployment_config }}"
      - debug:
          msg: Completed cleanup of Cluster B. Migrator Deployment deleted and Backup brough back up

      - debug:
          msg:
            - Backup now requires a restore maneuver. This must be done manually.
            - run 
            - BACKUP_POD=$(oc -n {{ migrator_vars.cluster_b.namespace }} get pods -o name | grep {{ migrator_vars.cluster_b.backup_deployment_config  }} | head -n 1 |  awk '{print $1}')
            - oc rsh -n {{ migrator_vars.cluster_b.namespace }} $BACKUP_POD
      # - debug:
      #     msg: Performing Backup Restore in Cluster B's Patroni Instance
      
      # todo 
      # - backup restore
      # - check patroni for lag
      # - scale up redhat sso 

      - pause:
          prompt: Have you completed a backup restore and would like to continue with scaling up the redhat sso service (yes/no)
        register: should_continue

      - name: Logging back into Cluster B to scale up SSO Service 
        shell: oc config  use-context {{ migrator_vars.cluster_b.context }}
        when: should_continue.user_input | bool

      - name: Scaling SSO Service up in Cluster B
        include_tasks: ./tasks/scale_up_sso.yml
        vars:
          TARGET_NAMESPACE: "{{ migrator_vars.cluster_b.namespace }}"
          SSO_DC_NAME: "{{ migrator_vars.cluster_b.sso_deployment_config }}"
          include_hpa: false
        when: should_continue.user_input | bool
        